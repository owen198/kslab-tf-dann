{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cPickle as pkl\n",
    "from sklearn.datasets import make_moons, make_blobs\n",
    "from sklearn.decomposition import PCA\n",
    "from IPython.display import SVG\n",
    "\n",
    "from flip_gradient import flip_gradient\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>active_num_days</th>\n",
       "      <th>active_sum_count</th>\n",
       "      <th>video_num_days</th>\n",
       "      <th>num_watched</th>\n",
       "      <th>num_complete</th>\n",
       "      <th>seek_video_sum</th>\n",
       "      <th>pause_video_sum</th>\n",
       "      <th>stop_video_sum</th>\n",
       "      <th>video_sum_count</th>\n",
       "      <th>video_forward_seek_sum</th>\n",
       "      <th>video_backward_seek_sum</th>\n",
       "      <th>video_pause_sum</th>\n",
       "      <th>video_stop_sum</th>\n",
       "      <th>mt_practice_sum</th>\n",
       "      <th>mt_unit_sum</th>\n",
       "      <th>mt_online_num_day</th>\n",
       "      <th>mt_online_practice_num_day</th>\n",
       "      <th>hw_mean</th>\n",
       "      <th>qz_mean</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.694878</td>\n",
       "      <td>250.927517</td>\n",
       "      <td>0.963108</td>\n",
       "      <td>3.402778</td>\n",
       "      <td>2.200521</td>\n",
       "      <td>1.947049</td>\n",
       "      <td>2.442708</td>\n",
       "      <td>1.920139</td>\n",
       "      <td>40.717014</td>\n",
       "      <td>8.673611</td>\n",
       "      <td>4.546007</td>\n",
       "      <td>9.137587</td>\n",
       "      <td>2.147569</td>\n",
       "      <td>2.440972</td>\n",
       "      <td>0.874132</td>\n",
       "      <td>0.646267</td>\n",
       "      <td>1.016927</td>\n",
       "      <td>9.112760</td>\n",
       "      <td>7.177517</td>\n",
       "      <td>0.671875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.740153</td>\n",
       "      <td>139.416619</td>\n",
       "      <td>0.514742</td>\n",
       "      <td>2.113309</td>\n",
       "      <td>1.720143</td>\n",
       "      <td>1.352082</td>\n",
       "      <td>1.486368</td>\n",
       "      <td>1.637463</td>\n",
       "      <td>41.539594</td>\n",
       "      <td>10.900600</td>\n",
       "      <td>4.431297</td>\n",
       "      <td>7.468888</td>\n",
       "      <td>1.866321</td>\n",
       "      <td>1.534490</td>\n",
       "      <td>0.421515</td>\n",
       "      <td>0.345637</td>\n",
       "      <td>0.540188</td>\n",
       "      <td>1.684985</td>\n",
       "      <td>2.074006</td>\n",
       "      <td>0.471376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.277778</td>\n",
       "      <td>163.722222</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.597222</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.152778</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>12.222222</td>\n",
       "      <td>1.805556</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>3.888889</td>\n",
       "      <td>0.652778</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>9.322222</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.805556</td>\n",
       "      <td>244.444444</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>3.722222</td>\n",
       "      <td>2.166667</td>\n",
       "      <td>1.888889</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>29.111111</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>3.694444</td>\n",
       "      <td>7.222222</td>\n",
       "      <td>2.111111</td>\n",
       "      <td>2.444444</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.611111</td>\n",
       "      <td>6.805556</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.222222</td>\n",
       "      <td>328.458333</td>\n",
       "      <td>1.388889</td>\n",
       "      <td>5.055556</td>\n",
       "      <td>2.944444</td>\n",
       "      <td>3.013889</td>\n",
       "      <td>3.611111</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>55.944444</td>\n",
       "      <td>10.111111</td>\n",
       "      <td>6.138889</td>\n",
       "      <td>12.277778</td>\n",
       "      <td>3.055556</td>\n",
       "      <td>3.166667</td>\n",
       "      <td>1.222222</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>9.811111</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.222222</td>\n",
       "      <td>594.388889</td>\n",
       "      <td>2.111111</td>\n",
       "      <td>8.611111</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>6.222222</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>7.111111</td>\n",
       "      <td>220.500000</td>\n",
       "      <td>57.611111</td>\n",
       "      <td>21.333333</td>\n",
       "      <td>32.777778</td>\n",
       "      <td>8.222222</td>\n",
       "      <td>7.166667</td>\n",
       "      <td>1.611111</td>\n",
       "      <td>1.222222</td>\n",
       "      <td>1.944444</td>\n",
       "      <td>9.977778</td>\n",
       "      <td>9.888889</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       active_num_days  active_sum_count  video_num_days  num_watched  \\\n",
       "count       128.000000        128.000000      128.000000   128.000000   \n",
       "mean          1.694878        250.927517        0.963108     3.402778   \n",
       "std           0.740153        139.416619        0.514742     2.113309   \n",
       "min           0.000000          0.000000        0.000000     0.000000   \n",
       "25%           1.277778        163.722222        0.500000     1.597222   \n",
       "50%           1.805556        244.444444        1.166667     3.722222   \n",
       "75%           2.222222        328.458333        1.388889     5.055556   \n",
       "max           3.222222        594.388889        2.111111     8.611111   \n",
       "\n",
       "       num_complete  seek_video_sum  pause_video_sum  stop_video_sum  \\\n",
       "count    128.000000      128.000000       128.000000      128.000000   \n",
       "mean       2.200521        1.947049         2.442708        1.920139   \n",
       "std        1.720143        1.352082         1.486368        1.637463   \n",
       "min        0.000000        0.000000         0.000000        0.000000   \n",
       "25%        0.694444        0.833333         1.152778        0.611111   \n",
       "50%        2.166667        1.888889         2.500000        1.833333   \n",
       "75%        2.944444        3.013889         3.611111        2.666667   \n",
       "max        7.500000        6.222222         6.500000        7.111111   \n",
       "\n",
       "       video_sum_count  video_forward_seek_sum  video_backward_seek_sum  \\\n",
       "count       128.000000              128.000000               128.000000   \n",
       "mean         40.717014                8.673611                 4.546007   \n",
       "std          41.539594               10.900600                 4.431297   \n",
       "min           0.000000                0.000000                 0.000000   \n",
       "25%          12.222222                1.805556                 1.125000   \n",
       "50%          29.111111                5.555556                 3.694444   \n",
       "75%          55.944444               10.111111                 6.138889   \n",
       "max         220.500000               57.611111                21.333333   \n",
       "\n",
       "       video_pause_sum  video_stop_sum  mt_practice_sum  mt_unit_sum  \\\n",
       "count       128.000000      128.000000       128.000000   128.000000   \n",
       "mean          9.137587        2.147569         2.440972     0.874132   \n",
       "std           7.468888        1.866321         1.534490     0.421515   \n",
       "min           0.000000        0.000000         0.000000     0.000000   \n",
       "25%           3.888889        0.652778         1.500000     0.611111   \n",
       "50%           7.222222        2.111111         2.444444     0.944444   \n",
       "75%          12.277778        3.055556         3.166667     1.222222   \n",
       "max          32.777778        8.222222         7.166667     1.611111   \n",
       "\n",
       "       mt_online_num_day  mt_online_practice_num_day     hw_mean     qz_mean  \\\n",
       "count         128.000000                  128.000000  128.000000  128.000000   \n",
       "mean            0.646267                    1.016927    9.112760    7.177517   \n",
       "std             0.345637                    0.540188    1.684985    2.074006   \n",
       "min             0.000000                    0.000000    0.000000    0.000000   \n",
       "25%             0.444444                    0.666667    9.322222    6.000000   \n",
       "50%             0.611111                    1.000000    9.611111    6.805556   \n",
       "75%             0.944444                    1.500000    9.811111    9.000000   \n",
       "max             1.222222                    1.944444    9.977778    9.888889   \n",
       "\n",
       "          cluster  \n",
       "count  128.000000  \n",
       "mean     0.671875  \n",
       "std      0.471376  \n",
       "min      0.000000  \n",
       "25%      0.000000  \n",
       "50%      1.000000  \n",
       "75%      1.000000  \n",
       "max      1.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features_by_week = pd.read_csv('ncu_data_week_1-18(3a).csv', sep=',')\n",
    "all_features_by_week.drop(['username'], axis=1, inplace=True)\n",
    "for i in range(128-59):\n",
    "    all_features_by_week = all_features_by_week.append(all_features_by_week.sample(1))\n",
    "\n",
    "\n",
    "feature_header = list(all_features_by_week)[1 : 20]\n",
    "cluster_header = 'cluster'\n",
    "Xt = all_features_by_week[feature_header].values\n",
    "yt = all_features_by_week[cluster_header].values\n",
    "all_features_by_week.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>active_num_days</th>\n",
       "      <th>active_sum_count</th>\n",
       "      <th>video_num_days</th>\n",
       "      <th>num_watched</th>\n",
       "      <th>num_complete</th>\n",
       "      <th>seek_video_sum</th>\n",
       "      <th>pause_video_sum</th>\n",
       "      <th>stop_video_sum</th>\n",
       "      <th>video_sum_count</th>\n",
       "      <th>video_forward_seek_sum</th>\n",
       "      <th>video_backward_seek_sum</th>\n",
       "      <th>video_pause_sum</th>\n",
       "      <th>video_stop_sum</th>\n",
       "      <th>mt_practice_sum</th>\n",
       "      <th>mt_unit_sum</th>\n",
       "      <th>mt_online_num_day</th>\n",
       "      <th>mt_online_practice_num_day</th>\n",
       "      <th>hw_mean</th>\n",
       "      <th>qz_mean</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.187500</td>\n",
       "      <td>2355.359375</td>\n",
       "      <td>7.375000</td>\n",
       "      <td>28.796875</td>\n",
       "      <td>21.242188</td>\n",
       "      <td>15.429688</td>\n",
       "      <td>26.203125</td>\n",
       "      <td>4.703125</td>\n",
       "      <td>251.070312</td>\n",
       "      <td>60.710938</td>\n",
       "      <td>39.445312</td>\n",
       "      <td>115.109375</td>\n",
       "      <td>4.828125</td>\n",
       "      <td>9.367188</td>\n",
       "      <td>4.164062</td>\n",
       "      <td>4.007812</td>\n",
       "      <td>5.390625</td>\n",
       "      <td>1.785896</td>\n",
       "      <td>42.769531</td>\n",
       "      <td>0.218750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.674108</td>\n",
       "      <td>2106.327340</td>\n",
       "      <td>6.304604</td>\n",
       "      <td>24.206578</td>\n",
       "      <td>20.644629</td>\n",
       "      <td>17.049573</td>\n",
       "      <td>23.083675</td>\n",
       "      <td>5.201997</td>\n",
       "      <td>328.536919</td>\n",
       "      <td>138.537306</td>\n",
       "      <td>68.308103</td>\n",
       "      <td>141.995355</td>\n",
       "      <td>5.390440</td>\n",
       "      <td>15.962491</td>\n",
       "      <td>5.462510</td>\n",
       "      <td>6.175257</td>\n",
       "      <td>7.767522</td>\n",
       "      <td>2.799147</td>\n",
       "      <td>30.621016</td>\n",
       "      <td>0.415023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>544.250000</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.750000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.375000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.500000</td>\n",
       "      <td>1876.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.286765</td>\n",
       "      <td>36.750000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>3522.250000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>49.250000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>42.750000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>379.250000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>159.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>10.250000</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>2.426471</td>\n",
       "      <td>71.458333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>10910.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>2444.000000</td>\n",
       "      <td>1153.000000</td>\n",
       "      <td>483.000000</td>\n",
       "      <td>653.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>100.833333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       active_num_days  active_sum_count  video_num_days  num_watched  \\\n",
       "count       128.000000        128.000000      128.000000   128.000000   \n",
       "mean         10.187500       2355.359375        7.375000    28.796875   \n",
       "std           8.674108       2106.327340        6.304604    24.206578   \n",
       "min           0.000000          0.000000        0.000000     0.000000   \n",
       "25%           3.000000        544.250000        2.750000     9.000000   \n",
       "50%           7.500000       1876.000000        6.000000    22.000000   \n",
       "75%          15.000000       3522.250000       11.000000    49.250000   \n",
       "max          34.000000      10910.000000       24.000000   127.000000   \n",
       "\n",
       "       num_complete  seek_video_sum  pause_video_sum  stop_video_sum  \\\n",
       "count    128.000000      128.000000       128.000000      128.000000   \n",
       "mean      21.242188       15.429688        26.203125        4.703125   \n",
       "std       20.644629       17.049573        23.083675        5.201997   \n",
       "min        0.000000        0.000000         0.000000        0.000000   \n",
       "25%        4.000000        2.750000         8.000000        0.000000   \n",
       "50%       15.000000       11.000000        19.000000        3.000000   \n",
       "75%       35.000000       26.000000        42.750000        7.000000   \n",
       "max      114.000000      107.000000       118.000000       26.000000   \n",
       "\n",
       "       video_sum_count  video_forward_seek_sum  video_backward_seek_sum  \\\n",
       "count       128.000000              128.000000               128.000000   \n",
       "mean        251.070312               60.710938                39.445312   \n",
       "std         328.536919              138.537306                68.308103   \n",
       "min           0.000000                0.000000                 0.000000   \n",
       "25%          34.750000                3.000000                 2.000000   \n",
       "50%         128.000000               16.500000                14.000000   \n",
       "75%         379.250000               50.000000                50.000000   \n",
       "max        2444.000000             1153.000000               483.000000   \n",
       "\n",
       "       video_pause_sum  video_stop_sum  mt_practice_sum  mt_unit_sum  \\\n",
       "count       128.000000      128.000000       128.000000   128.000000   \n",
       "mean        115.109375        4.828125         9.367188     4.164062   \n",
       "std         141.995355        5.390440        15.962491     5.462510   \n",
       "min           0.000000        0.000000         0.000000     0.000000   \n",
       "25%          15.750000        0.000000         0.000000     0.000000   \n",
       "50%          61.000000        3.000000         2.000000     2.000000   \n",
       "75%         159.500000        7.000000        10.250000     5.250000   \n",
       "max         653.000000       27.000000        87.000000    18.000000   \n",
       "\n",
       "       mt_online_num_day  mt_online_practice_num_day     hw_mean     qz_mean  \\\n",
       "count         128.000000                  128.000000  128.000000  128.000000   \n",
       "mean            4.007812                    5.390625    1.785896   42.769531   \n",
       "std             6.175257                    7.767522    2.799147   30.621016   \n",
       "min             0.000000                    0.000000    0.000000    0.000000   \n",
       "25%             0.000000                    0.000000    0.000000   15.375000   \n",
       "50%             1.000000                    2.000000    0.286765   36.750000   \n",
       "75%             5.250000                    6.250000    2.426471   71.458333   \n",
       "max            34.000000                   31.000000   10.000000  100.833333   \n",
       "\n",
       "          cluster  \n",
       "count  128.000000  \n",
       "mean     0.218750  \n",
       "std      0.415023  \n",
       "min      0.000000  \n",
       "25%      0.000000  \n",
       "50%      0.000000  \n",
       "75%      0.000000  \n",
       "max      1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all_features_by_week = pd.read_csv('ncu_data_week_chkb_1-6(59).csv', sep=',')\n",
    "all_features_by_week = pd.read_csv('ncu_data_week_1-6.csv', sep=',')\n",
    "feature_header = list(all_features_by_week)[1 : 20]\n",
    "cluster_header = 'cluster'\n",
    "Xs = all_features_by_week[feature_header].values\n",
    "ys = all_features_by_week[cluster_header].values\n",
    "all_features_by_week.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f26042947d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f26043c6050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# source: yellow and purple\n",
    "plt.scatter(Xs[:,0], Xs[:,1], c=ys, alpha=0.4) \n",
    "\n",
    "# target: light blue and pink\n",
    "plt.scatter(Xt[:,0], Xt[:,1], c=yt, cmap='cool', alpha=0.4, label='target')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = 16\n",
    "batch_size = 49\n",
    "\n",
    "def build_model(shallow_domain_classifier=True):\n",
    "    X = tf.placeholder(tf.float32, [None, 19], name='X') # Input data\n",
    "    Y_index = tf.placeholder(tf.int32, [None], name='Y_index')  # Class index\n",
    "    D_index = tf.placeholder(tf.int32, [None], name='D_index')  # Domain index\n",
    "    train = tf.placeholder(tf.bool, [], name='train')       # Switch for routing data to class predictor\n",
    "    l = tf.placeholder(tf.float32, [], name='l')        # Gradient reversal scaler\n",
    "    print 'X', X\n",
    "    print 'train', train\n",
    "    print 'l', l\n",
    "    \n",
    "    Y = tf.one_hot(Y_index, 2)\n",
    "    D = tf.one_hot(D_index, 2)\n",
    "\n",
    "    # Feature extractor - single layer\n",
    "    W0 = weight_variable([19, 38])\n",
    "    b0 = bias_variable([38])\n",
    "    F = tf.nn.relu(tf.matmul(X, W0) + b0, name='feature')\n",
    "    print tf.slice(F, [0, 0], [batch_size / 2, -1])\n",
    "    print 'F', F\n",
    "    \n",
    "    # Label predictor - single layer\n",
    "    f = tf.cond(train, lambda: tf.slice(F, [0, 0], [batch_size / 2, -1]), lambda: F)\n",
    "    y = tf.cond(train, lambda: tf.slice(Y, [0, 0], [batch_size / 2, -1]), lambda: Y)\n",
    "\n",
    "    W1 = weight_variable([38, 2])\n",
    "    b1 = bias_variable([2])\n",
    "    p_logit = tf.matmul(f, W1) + b1\n",
    "    p = tf.nn.softmax(p_logit)\n",
    "    p_loss = tf.nn.softmax_cross_entropy_with_logits(logits=p_logit, labels=y)\n",
    "\n",
    "    print 'p_logit', p_logit\n",
    "    \n",
    "    # Domain predictor - shallow\n",
    "    f_ = flip_gradient(F, l)\n",
    "\n",
    "    if shallow_domain_classifier:\n",
    "        W2 = weight_variable([38, 2])\n",
    "        b2 = bias_variable([2])\n",
    "        d_logit = tf.matmul(f_, W2) + b2\n",
    "        d = tf.nn.softmax(d_logit)\n",
    "        d_loss = tf.nn.softmax_cross_entropy_with_logits(logits=d_logit, labels=D)\n",
    "\n",
    "    else:\n",
    "        W2 = weight_variable([38, 8])\n",
    "        b2 = bias_variable([8])\n",
    "        h2 = tf.nn.relu(tf.matmul(f_, W2) + b2)\n",
    "\n",
    "        W3 = weight_variable([8, 2])\n",
    "        b3 = bias_variable([2])\n",
    "        d_logit = tf.matmul(h2, W3) + b3\n",
    "        d = tf.nn.softmax(d_logit)\n",
    "        d_loss = tf.nn.softmax_cross_entropy_with_logits(logits=d_logit, labels=D)\n",
    "\n",
    "\n",
    "    print 'd_logit\\t', d_logit\n",
    "        \n",
    "    # Optimization\n",
    "    pred_loss = tf.reduce_sum(p_loss, name='pred_loss')\n",
    "    domain_loss = tf.reduce_sum(d_loss, name='domain_loss')\n",
    "    total_loss = tf.add(pred_loss, domain_loss, name='total_loss')\n",
    "\n",
    "    pred_train_op = tf.train.AdamOptimizer(0.01).minimize(pred_loss, name='pred_train_op')\n",
    "    domain_train_op = tf.train.AdamOptimizer(0.01).minimize(domain_loss, name='domain_train_op')\n",
    "    dann_train_op = tf.train.AdamOptimizer(0.01).minimize(total_loss, name='dann_train_op')\n",
    "\n",
    "    # Evaluation\n",
    "    p_acc = tf.reduce_mean(tf.cast(tf.equal(tf.arg_max(y, 1), tf.arg_max(p, 1)), tf.float32), name='p_acc')\n",
    "    d_acc = tf.reduce_mean(tf.cast(tf.equal(tf.arg_max(D, 1), tf.arg_max(d, 1)), tf.float32), name='d_acc')\n",
    "    \n",
    "    #writer = tf.train.write_graph(sess.graph_def, '/notebooks/tf-dann/model-test', 'train.pb')\n",
    "    #writer = tf.summary.FileWriter('/notebooks/tf-dann/model-test', graph = sess.graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Tensor(\"X:0\", shape=(?, 19), dtype=float32)\n",
      "train Tensor(\"train:0\", shape=(), dtype=bool)\n",
      "l Tensor(\"l:0\", shape=(), dtype=float32)\n",
      "Tensor(\"Slice:0\", shape=(24, 38), dtype=float32)\n",
      "F Tensor(\"feature:0\", shape=(?, 38), dtype=float32)\n",
      "p_logit Tensor(\"add_1:0\", shape=(?, 2), dtype=float32)\n",
      "d_logit\tTensor(\"add_2:0\", shape=(?, 2), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-5-7c23a908abf3>:70: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `argmax` instead\n"
     ]
    }
   ],
   "source": [
    "\n",
    "build_model()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "def train_and_evaluate(sess, \n",
    "                       train_op_name, \n",
    "                       train_loss_name, \n",
    "                       model='dann', \n",
    "                       grad_scale=None, \n",
    "                       num_batches=10000, \n",
    "                       verbose=True):\n",
    "    \n",
    "    # Create batch builders\n",
    "    S_batches = batch_generator([Xs, ys], batch_size / 2)\n",
    "    T_batches = batch_generator([Xt, yt], batch_size / 2)\n",
    "    \n",
    "    # Get output tensors and train op\n",
    "    d_acc = sess.graph.get_tensor_by_name('d_acc:0')\n",
    "    p_acc = sess.graph.get_tensor_by_name('p_acc:0')\n",
    "    train_loss = sess.graph.get_tensor_by_name(train_loss_name + ':0')\n",
    "    train_op = sess.graph.get_operation_by_name(train_op_name)\n",
    "    \n",
    "    \n",
    "    writer = tf.summary.FileWriter('/notebooks/tf-dann/model-test', graph = sess.graph)\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    for i in range(num_batches):\n",
    "\n",
    "        # If no grad_scale, use a schedule\n",
    "        if grad_scale is None:\n",
    "            p = float(i) / num_batches\n",
    "            lp = 2. / (1. + np.exp(-10. * p)) - 1\n",
    "        else:\n",
    "            lp = grad_scale\n",
    "\n",
    "        if model == 'dann':\n",
    "            X0, y0 = S_batches.next()\n",
    "            X1, y1 = T_batches.next()\n",
    "\n",
    "            Xb = np.vstack([X0, X1])\n",
    "            yb = np.hstack([y0, y1])\n",
    "            D_labels = np.hstack([np.zeros(batch_size / 2, dtype=np.int32),\n",
    "                                  np.ones(batch_size / 2, dtype=np.int32)])\n",
    "\n",
    "            _, loss, da, pa = sess.run([train_op, train_loss, d_acc, p_acc],\n",
    "                                       feed_dict={'X:0': Xb, \n",
    "                                                  'Y_index:0': yb, \n",
    "                                                  'D_index:0': D_labels,\n",
    "                                                  'train:0': True, \n",
    "                                                  'l:0': lp})\n",
    "\n",
    "            if verbose and i % 2000 == 0:\n",
    "                print 'loss: %f, domain accuracy: %f, class accuracy: %f' % (loss, da, pa)\n",
    "                \n",
    "        elif model == 'source':\n",
    "            X0, y0 = S_batches.next()\n",
    "            Xb = np.vstack([X0])\n",
    "            yb = np.hstack([y0])\n",
    "            D_labels = np.hstack([np.zeros(batch_size / 4, dtype=np.int32),\n",
    "                                  np.ones(batch_size / 4, dtype=np.int32)])\n",
    "            \n",
    "            _, loss, da, pa = sess.run([train_op, train_loss, d_acc, p_acc],\n",
    "                                       feed_dict={'X:0': Xb, \n",
    "                                                  'Y_index:0': yb, \n",
    "                                                  'D_index:0': D_labels,\n",
    "                                                  'train:0': True, \n",
    "                                                  'l:0': lp})\n",
    "\n",
    "            \n",
    "    # Get final accuracies on whole dataset\n",
    "    das, pas = sess.run([d_acc, p_acc], feed_dict={'X:0': Xs, \n",
    "                                                   'Y_index:0': ys, \n",
    "                                                   'D_index:0': np.zeros(Xs.shape[0], dtype=np.int32), \n",
    "                                                   'train:0': False, \n",
    "                                                   'l:0': 1.0})\n",
    "    dat, pat = sess.run([d_acc, p_acc], feed_dict={'X:0': Xt, \n",
    "                                                   'Y_index:0': yt,\n",
    "                                                   'D_index:0': np.ones(Xt.shape[0], dtype=np.int32), \n",
    "                                                   'train:0': False, \n",
    "                                                   'l:0': 1.0})\n",
    "    dax, pax = sess.run([d_acc, p_acc], feed_dict={'X:0': Xt, \n",
    "                                                   'Y_index:0': ys,\n",
    "                                                   'D_index:0': np.ones(Xt.shape[0], dtype=np.int32), \n",
    "                                                   'train:0': False, \n",
    "                                                   'l:0': 1.0})\n",
    "    \n",
    "    \n",
    "    return pas, pat, pax\n",
    "\n",
    "def extract_and_plot_pca_feats(sess, feat_tensor_name='feature'):\n",
    "    F = sess.graph.get_tensor_by_name(feat_tensor_name + ':0')\n",
    "    emb_s = sess.run(F, feed_dict={'X:0': Xs})\n",
    "    emb_t = sess.run(F, feed_dict={'X:0': Xt})\n",
    "    emb_all = np.vstack([emb_s, emb_t])\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_emb = pca.fit_transform(emb_all)\n",
    "\n",
    "    num = pca_emb.shape[0] / 2\n",
    "    plt.scatter(pca_emb[:num,0], pca_emb[:num,1], c=ys, alpha=0.4)\n",
    "    plt.scatter(pca_emb[num:,0], pca_emb[num:,1], c=yt, cmap='cool', alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Source only training\n",
      "Source (Senior) accuracy: (Apply source model to source) 1.0\n",
      "Target (College) accuracy: (Apply source model to target) 0.328125\n",
      "\n",
      "Domain adaptation training\n",
      "Source (Senior) accuracy (Apply domain model to source): 0.796875\n",
      "Target (College) accuracy (Apply domain model to target): 0.773438\n",
      "Domain classification accuracy: 0.273438\n"
     ]
    }
   ],
   "source": [
    "print '\\nSource only training'\n",
    "source_acc, target_acc, _ = train_and_evaluate(sess, \n",
    "                                               'pred_train_op', \n",
    "                                               'pred_loss', \n",
    "                                               verbose=False, \n",
    "                                               model='source',\n",
    "                                               num_batches=10000)\n",
    "print 'Source (Senior) accuracy: (Apply source model to source)', source_acc\n",
    "print 'Target (College) accuracy: (Apply source model to target)', target_acc\n",
    "\n",
    "print '\\nDomain adaptation training'\n",
    "source_acc, target_acc, d_acc = train_and_evaluate(sess, \n",
    "                                                   'dann_train_op', \n",
    "                                                   'total_loss', \n",
    "                                                   verbose=False, \n",
    "                                                   model='dann',\n",
    "                                                   num_batches=5000)\n",
    "\n",
    "print 'Source (Senior) accuracy (Apply domain model to source):', source_acc\n",
    "print 'Target (College) accuracy (Apply domain model to target):', target_acc\n",
    "print 'Domain classification accuracy:', d_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Source only training\n",
    "+ Source (MNIST) accuracy: 0.9847\n",
    "+ Target (MNIST-M) accuracy: 0.5463\n",
    "\n",
    "Domain adaptation training\n",
    "+ Source (MNIST) accuracy: 0.9779\n",
    "+ Target (MNIST-M) accuracy: 0.7141\n",
    "+ Domain accuracy: 0.693"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
